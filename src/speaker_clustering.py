"""
KonuÅŸmacÄ± kÃ¼meleme, kimlik atama ve benzerlik analizi
"""
import numpy as np
from sklearn.cluster import DBSCAN, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances
from typing import List, Dict, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

class SpeakerClustering:
    def __init__(self, algorithm: str = "dbscan", min_speakers: int = 1, max_speakers: int = 10, 
                 similarity_threshold: float = 0.6):
        self.algorithm = algorithm
        self.min_speakers = min_speakers
        self.max_speakers = max_speakers
        self.similarity_threshold = similarity_threshold
        self.scaler = StandardScaler()
        self.labels_ = None
        self.n_speakers_ = 0
        self.embeddings_ = {}
        self.similarity_matrix_ = {}
        print(f"ğŸ”§ SpeakerClustering baÅŸlatÄ±ldÄ± - Algoritma: {algorithm}")
    
    def cluster_speakers(self, features_list: List[np.ndarray]) -> np.ndarray:
        """
        KonuÅŸmacÄ±larÄ± Ã¶zelliklere gÃ¶re kÃ¼mele
        """
        try:
            print("ğŸ”„ KonuÅŸmacÄ± kÃ¼meleme baÅŸlÄ±yor...")
            
            if len(features_list) < 2:
                print("âš ï¸  Yeterli segment yok, tÃ¼m segmentler aynÄ± konuÅŸmacÄ±ya atanacak")
                return np.zeros(len(features_list), dtype=int)
            
            # Ã–zellik matrisini oluÅŸtur
            X = np.array(features_list)
            print(f"   Ã–zellik matrisi boyutu: {X.shape}")
            
            # NaN ve sonsuz deÄŸerleri temizle
            X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
            
            # Ã–lÃ§eklendirme
            X_scaled = self.scaler.fit_transform(X)
            
            if self.algorithm == "dbscan":
                labels = self._dbscan_clustering(X_scaled)
            elif self.algorithm == "hierarchical":
                labels = self._hierarchical_clustering(X_scaled)
            else:
                labels = self._auto_clustering(X_scaled)
            
            self.labels_ = labels
            
            # -1 etiketlerini (gÃ¼rÃ¼ltÃ¼) en yakÄ±n kÃ¼meye ata
            labels = self._fix_noise_labels(labels, X_scaled)
            
            unique_labels = np.unique(labels)
            self.n_speakers_ = len(unique_labels)
            
            print(f"âœ… KÃ¼meleme tamamlandÄ± - {self.n_speakers_} konuÅŸmacÄ± tespit edildi")
            
            # Ä°statistikleri gÃ¶ster
            self._print_cluster_stats(labels)
            
            return labels
            
        except Exception as e:
            print(f"âŒ KÃ¼meleme hatasÄ±: {e}")
            # Hata durumunda tÃ¼m segmentleri aynÄ± konuÅŸmacÄ± yap
            return np.zeros(len(features_list), dtype=int)
    
    def _dbscan_clustering(self, X: np.ndarray) -> np.ndarray:
        """DBSCAN algoritmasÄ± ile kÃ¼meleme - Daha az strict"""
        # Daha geniÅŸ parametreler
        eps = 0.8  # Daha geniÅŸ komÅŸuluk
        min_samples = max(2, min(3, len(X) // 15))  # Daha az min_samples
        
        clustering = DBSCAN(
            eps=eps,
            min_samples=min_samples,
            metric='euclidean'
        ).fit(X)
        
        labels = clustering.labels_
        
        # EÄŸer Ã§ok fazla gÃ¼rÃ¼ltÃ¼ varsa, parametreleri gevÅŸet
        noise_ratio = np.sum(labels == -1) / len(labels)
        if noise_ratio > 0.7:  # %70'ten fazla gÃ¼rÃ¼ltÃ¼ varsa
            print("   âš ï¸  Ã‡ok fazla gÃ¼rÃ¼ltÃ¼, parametreler gevÅŸetiliyor...")
            clustering = DBSCAN(
                eps=1.0,  # Daha geniÅŸ
                min_samples=2,  # Minimum
                metric='euclidean'
            ).fit(X)
            labels = clustering.labels_
        
        return labels
    
    def _hierarchical_clustering(self, X: np.ndarray) -> np.ndarray:
        """HiyerarÅŸik kÃ¼meleme"""
        # Optimal kÃ¼me sayÄ±sÄ±nÄ± bul
        n_clusters = self._find_optimal_clusters(X)
        
        clustering = AgglomerativeClustering(
            n_clusters=n_clusters,
            metric='euclidean',
            linkage='average'
        ).fit(X)
        
        return clustering.labels_
    
    def _auto_clustering(self, X: np.ndarray) -> np.ndarray:
        """Otomatik algoritma seÃ§imi"""
        if len(X) < 5:
            return self._hierarchical_clustering(X)
        else:
            # Ã–nce DBSCAN dene
            labels = self._dbscan_clustering(X)
            unique_labels = np.unique(labels)
            
            # EÄŸer Ã§ok fazla gÃ¼rÃ¼ltÃ¼ varsa veya hiÃ§ kÃ¼me yoksa, hiyerarÅŸik kullan
            if len(unique_labels) <= 1 or np.sum(labels == -1) / len(labels) > 0.5:
                print("   âš ï¸  DBSCAN baÅŸarÄ±sÄ±z, hiyerarÅŸik kÃ¼meleme kullanÄ±lÄ±yor...")
                return self._hierarchical_clustering(X)
            
            return labels
    
    def _fix_noise_labels(self, labels: np.ndarray, X: np.ndarray) -> np.ndarray:
        """GÃ¼rÃ¼ltÃ¼ etiketlerini en yakÄ±n kÃ¼meye ata"""
        noise_indices = np.where(labels == -1)[0]
        cluster_indices = np.where(labels != -1)[0]
        
        if len(noise_indices) == 0 or len(cluster_indices) == 0:
            return labels
        
        # GÃ¼rÃ¼ltÃ¼ noktalarÄ±nÄ± en yakÄ±n kÃ¼meye ata
        from sklearn.neighbors import NearestNeighbors
        
        # KÃ¼me merkezlerini bul
        unique_labels = np.unique(labels[cluster_indices])
        centers = []
        for label in unique_labels:
            cluster_points = X[labels == label]
            centers.append(np.mean(cluster_points, axis=0))
        centers = np.array(centers)
        
        # Her gÃ¼rÃ¼ltÃ¼ noktasÄ± iÃ§in en yakÄ±n merkezi bul
        nbrs = NearestNeighbors(n_neighbors=1).fit(centers)
        distances, indices = nbrs.kneighbors(X[noise_indices])
        
        # Etiketleri ata
        fixed_labels = labels.copy()
        for i, idx in enumerate(noise_indices):
            fixed_labels[idx] = unique_labels[indices[i][0]]
        
        if len(noise_indices) > 0:
            print(f"   ğŸ”§ {len(noise_indices)} gÃ¼rÃ¼ltÃ¼ segmenti en yakÄ±n kÃ¼meye atandÄ±")
        
        return fixed_labels
    
    def _find_optimal_clusters(self, X: np.ndarray) -> int:
        """Optimal kÃ¼me sayÄ±sÄ±nÄ± bul"""
        if len(X) <= 2:
            return 1
        
        max_clusters = min(self.max_speakers, len(X) - 1)
        min_clusters = max(1, self.min_speakers)
        
        # KÃ¼Ã§Ã¼k datasetler iÃ§in basit yaklaÅŸÄ±m
        if len(X) <= 10:
            return min(3, max(1, len(X) // 3))
        
        best_score = -1
        best_n = 1
        
        for n in range(min_clusters, max_clusters + 1):
            if n >= len(X):
                break
                
            try:
                clustering = AgglomerativeClustering(n_clusters=n)
                labels = clustering.fit_predict(X)
                
                if len(np.unique(labels)) > 1:
                    score = silhouette_score(X, labels)
                    if score > best_score:
                        best_score = score
                        best_n = n
            except:
                continue
        
        # Minimum kalite eÅŸiÄŸi dÃ¼ÅŸÃ¼rÃ¼ldÃ¼
        return best_n if best_score > 0.1 else min(2, len(X))  # Daha dÃ¼ÅŸÃ¼k eÅŸik
    
    def advanced_speaker_detection(self, features_list: List[np.ndarray]) -> Dict[str, Any]:
        """
        GeliÅŸmiÅŸ konuÅŸmacÄ± tespiti ve benzerlik analizi
        """
        try:
            print("ğŸ¯ GeliÅŸmiÅŸ konuÅŸmacÄ± analizi baÅŸlÄ±yor...")
            
            # 1. Temel kÃ¼meleme
            labels = self.cluster_speakers(features_list)
            
            # 2. KonuÅŸmacÄ± embedding'lerini hesapla
            embeddings = self.calculate_speaker_embeddings(features_list, labels)
            self.embeddings_ = embeddings
            
            # 3. Benzerlik matrisini hesapla
            similarities = {}
            if len(embeddings) > 1:
                similarities = self.compare_all_speakers(embeddings)
            self.similarity_matrix_ = similarities
            
            # 4. AynÄ± kiÅŸi olabilecek konuÅŸmacÄ±larÄ± tespit et
            same_speaker_pairs = self.find_same_speakers(similarities) if similarities else []
            
            # 5. SonuÃ§larÄ± birleÅŸtir
            result = {
                'speaker_count': len(embeddings),
                'speaker_ids': list(embeddings.keys()),
                'cluster_labels': labels.tolist(),
                'embeddings': {k: v.tolist() for k, v in embeddings.items()},
                'similarity_matrix': similarities,
                'same_speaker_pairs': same_speaker_pairs,
                'cluster_stats': self.get_cluster_stats(labels)
            }
            
            # 6. Analiz raporunu yazdÄ±r
            self._print_advanced_analysis(result)
            
            return result
            
        except Exception as e:
            print(f"âŒ GeliÅŸmiÅŸ analiz hatasÄ±: {e}")
            # Hata durumunda basit sonuÃ§ dÃ¶ndÃ¼r
            labels = np.zeros(len(features_list), dtype=int)
            return {
                'speaker_count': 1,
                'speaker_ids': ['SPK_01'],
                'cluster_labels': labels.tolist(),
                'embeddings': {'SPK_01': np.mean(features_list, axis=0).tolist()},
                'similarity_matrix': {},
                'same_speaker_pairs': [],
                'cluster_stats': self.get_cluster_stats(labels)
            }
    
    def calculate_speaker_similarity(self, features1: np.ndarray, features2: np.ndarray) -> float:
        """
        Ä°ki konuÅŸmacÄ± arasÄ±ndaki benzerlik skorunu hesapla
        """
        try:
            # KosinÃ¼s benzerliÄŸi
            similarity = cosine_similarity([features1], [features2])[0][0]
            
            # 0-1 aralÄ±ÄŸÄ±na normalize et
            similarity = max(0.0, min(1.0, similarity))
            
            return float(similarity)
            
        except:
            try:
                # Ã–klid mesafesi tabanlÄ± benzerlik (yedek yÃ¶ntem)
                distance = np.linalg.norm(features1 - features2)
                similarity = 1.0 / (1.0 + distance)
                return float(similarity)
            except:
                return 0.0
    
    def compare_all_speakers(self, embeddings: Dict[str, np.ndarray]) -> Dict[str, Any]:
        """
        TÃ¼m konuÅŸmacÄ± Ã§iftleri arasÄ±ndaki benzerlikleri hesapla
        """
        similarities = {}
        speaker_ids = list(embeddings.keys())
        
        print(f"   ğŸ” {len(speaker_ids)} konuÅŸmacÄ± arasÄ±ndaki benzerlikler hesaplanÄ±yor...")
        
        for i, spk1 in enumerate(speaker_ids):
            for j, spk2 in enumerate(speaker_ids):
                if i < j:  # AynÄ± Ã§ifti tekrar hesaplama
                    key = f"{spk1}_vs_{spk2}"
                    similarity = self.calculate_speaker_similarity(
                        embeddings[spk1], embeddings[spk2]
                    )
                    
                    similarities[key] = {
                        'similarity_score': similarity,
                        'same_speaker': similarity > self.similarity_threshold,
                        'similarity_percentage': round(similarity * 100, 1)
                    }
        
        return similarities
    
    def find_same_speakers(self, similarities: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        AynÄ± kiÅŸi olabilecek konuÅŸmacÄ± Ã§iftlerini bul
        """
        same_speakers = []
        
        for pair, data in similarities.items():
            if data['same_speaker']:
                spk1, spk2 = pair.split('_vs_')
                same_speakers.append({
                    'speaker1': spk1,
                    'speaker2': spk2,
                    'similarity': data['similarity_score'],
                    'similarity_percentage': data['similarity_percentage']
                })
        
        return same_speakers
    
    def get_speaker_confidence(self, embeddings: Dict[str, np.ndarray], speaker_id: str) -> float:
        """
        KonuÅŸmacÄ± tespitinin gÃ¼ven skorunu hesapla
        """
        try:
            if speaker_id not in embeddings:
                return 0.5
            
            speaker_embedding = embeddings[speaker_id]
            other_embeddings = [emb for spk, emb in embeddings.items() if spk != speaker_id]
            
            if not other_embeddings:
                return 0.8  # Tek konuÅŸmacÄ± varsa orta gÃ¼ven
            
            # DiÄŸer konuÅŸmacÄ±larla olan benzerliklerin ortalamasÄ±
            similarities = []
            for other_emb in other_embeddings:
                sim = self.calculate_speaker_similarity(speaker_embedding, other_emb)
                similarities.append(sim)
            
            # DÃ¼ÅŸÃ¼k benzerlik = yÃ¼ksek gÃ¼ven
            avg_similarity = np.mean(similarities)
            confidence = 1.0 - avg_similarity
            
            return max(0.3, min(0.9, confidence))  # Min 0.3, max 0.9 gÃ¼ven
            
        except:
            return 0.5  # VarsayÄ±lan gÃ¼ven
    
    def assign_speaker_ids(self, labels: np.ndarray) -> List[str]:
        """
        KÃ¼me etiketlerini konuÅŸmacÄ± ID'lerine dÃ¶nÃ¼ÅŸtÃ¼r
        """
        speaker_ids = []
        unique_labels = np.unique(labels)
        
        # TÃ¼m etiketleri konuÅŸmacÄ± yap (gÃ¼rÃ¼ltÃ¼ yok)
        label_to_id = {}
        for i, label in enumerate(unique_labels):
            label_to_id[label] = f"SPK_{i+1:02d}"
        
        for label in labels:
            speaker_ids.append(label_to_id[label])
        
        return speaker_ids
    
    def calculate_speaker_embeddings(self, features: List[np.ndarray], labels: np.ndarray) -> Dict[str, np.ndarray]:
        """
        Her konuÅŸmacÄ± iÃ§in ortalama Ã¶zellik vektÃ¶rÃ¼ hesapla
        """
        embeddings = {}
        
        for speaker_id in np.unique(labels):
            speaker_features = [features[i] for i in range(len(features)) if labels[i] == speaker_id]
            if speaker_features and len(speaker_features) >= 1:
                embedding = np.mean(speaker_features, axis=0)
                speaker_key = f"SPK_{speaker_id+1:02d}"
                embeddings[speaker_key] = embedding
        
        return embeddings
    
    def get_cluster_stats(self, labels: np.ndarray) -> Dict[str, Any]:
        """
        KÃ¼me istatistiklerini hesapla
        """
        unique_labels = np.unique(labels)
        stats = {
            'total_segments': len(labels),
            'total_speakers': len(unique_labels),
            'segment_distribution': {},
            'speaker_segment_counts': {}
        }
        
        for label in unique_labels:
            count = np.sum(labels == label)
            speaker_id = f"SPK_{label+1:02d}"
            stats['segment_distribution'][speaker_id] = count
            stats['speaker_segment_counts'][speaker_id] = count
        
        return stats
    
    def _print_cluster_stats(self, labels: np.ndarray):
        """KÃ¼me istatistiklerini yazdÄ±r"""
        stats = self.get_cluster_stats(labels)
        
        print(f"   ğŸ“Š KÃ¼me Ä°statistikleri:")
        print(f"      Toplam Segment: {stats['total_segments']}")
        print(f"      KonuÅŸmacÄ± SayÄ±sÄ±: {stats['total_speakers']}")
        
        for speaker, count in stats['segment_distribution'].items():
            print(f"      {speaker}: {count} segment")
    
    def _print_advanced_analysis(self, result: Dict[str, Any]):
        """GeliÅŸmiÅŸ analiz raporunu yazdÄ±r"""
        print(f"\nğŸ¯ GELÄ°ÅMÄ°Å KONUÅMACI ANALÄ°Z RAPORU")
        print("=" * 50)
        
        print(f"ğŸ‘¥ TOPLAM KONUÅMACI: {result['speaker_count']}")
        
        # KonuÅŸmacÄ± detaylarÄ±
        if result['speaker_ids']:
            print(f"\nğŸ“‹ KONUÅMACI DETAYLARI:")
            for speaker_id in result['speaker_ids']:
                segment_count = result['cluster_stats']['speaker_segment_counts'].get(speaker_id, 0)
                print(f"   {speaker_id}: {segment_count} segment")
        else:
            print(f"\nğŸ“‹ KONUÅMACI DETAYLARI: HiÃ§ konuÅŸmacÄ± bulunamadÄ±")
        
        # Benzerlik analizi
        similarities = result['similarity_matrix']
        if similarities:
            print(f"\nğŸ” KONUÅMACI BENZERLÄ°K ANALÄ°ZÄ°:")
            for pair, data in similarities.items():
                similarity_pct = data['similarity_percentage']
                if data['same_speaker']:
                    print(f"   âš ï¸  {pair}: AYNI KÄ°ÅÄ° OLABÄ°LÄ°R (%{similarity_pct:.1f} benzer)")
                else:
                    if similarity_pct > 50:
                        print(f"   ğŸ”¸ {pair}: YÃ¼ksek Benzerlik (%{similarity_pct:.1f})")
        
        # AynÄ± kiÅŸi uyarÄ±larÄ±
        if result['same_speaker_pairs']:
            print(f"\nğŸš¨ UYARI: AYNI KÄ°ÅÄ° OLABÄ°LECEK KONUÅMACILAR:")
            for pair in result['same_speaker_pairs']:
                print(f"   â€¢ {pair['speaker1']} â†” {pair['speaker2']} (%{pair['similarity_percentage']:.1f} benzer)")

# Test fonksiyonu
if __name__ == "__main__":
    # Test iÃ§in rastgele Ã¶zellikler oluÅŸtur
    np.random.seed(42)
    
    # 2 farklÄ± konuÅŸmacÄ±yÄ± simÃ¼le et
    test_features = []
    for i in range(2):  # 2 konuÅŸmacÄ±
        base_feature = np.random.randn(50) * (i + 1)  # Her konuÅŸmacÄ± farklÄ±
        for _ in range(8):  # Her konuÅŸmacÄ± iÃ§in 8 segment
            # KÃ¼Ã§Ã¼k varyasyonlar ekle
            variation = np.random.normal(0, 0.2, 50)
            test_features.append(base_feature + variation)
    
    clusterer = SpeakerClustering(similarity_threshold=0.5)  # Daha dÃ¼ÅŸÃ¼k eÅŸik
    
    # GeliÅŸmiÅŸ analiz
    advanced_result = clusterer.advanced_speaker_detection(test_features)
    
    print("\n" + "="*50)
    print("âœ… KonuÅŸmacÄ± kÃ¼meleme testi baÅŸarÄ±lÄ±!")